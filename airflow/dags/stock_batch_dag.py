"""
Airflow DAG: nightly stock batch job.
Schedule 00:05 daily. Tasks: extract_raw_trades (no-op / placeholder), compute_ohlcv + generate_reports (batch_job).
SLA 60 minutes.
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import os

# Default args: SLA 60 min, catchup False
default_args = {
    "owner": "stock-analytics",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "sla": timedelta(minutes=60),
}

# Set BATCH_JOB_DIR and PG_* env in Airflow so the batch job can run.
# Example: mount repo and set BATCH_JOB_DIR=/app/batch-processing, PG_HOST=postgres, etc.
BATCH_JOB_DIR = os.environ.get("BATCH_JOB_DIR", "/app/batch-processing")

with DAG(
    dag_id="stock_nightly_batch",
    default_args=default_args,
    description="Nightly OHLCV aggregation and reports",
    schedule_interval="5 0 * * *",  # 00:05 daily
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["stock", "batch"],
) as dag:

    # Task 1: placeholder for extract (actual extract is in batch_job reading raw_trades)
    extract_task = BashOperator(
        task_id="extract_raw_trades",
        bash_command="echo 'Raw trades read from PostgreSQL in compute_ohlcv'",
    )

    compute_task = BashOperator(
        task_id="compute_ohlcv",
        bash_command=f"cd {BATCH_JOB_DIR} && python batch_job.py",
    )

    report_task = BashOperator(
        task_id="generate_reports",
        bash_command="echo 'Reports generated by compute_ohlcv'",
    )

    extract_task >> compute_task >> report_task
